---
version: "3"

vars:
  TALOS_DIR: talos
  TALOS_CONFIG: "{{.TALOS_DIR}}/clusterconfig/talosconfig"
  CLUSTER_NAME: going-merry
  NODES: "192.168.1.10,192.168.1.11,192.168.1.12"
  NODE_LUFFY: "192.168.1.10"
  NODE_ZORO: "192.168.1.11"
  NODE_NAMI: "192.168.1.12"
  CILIUM_VERSION: "1.18.5"

env:
  TALOSCONFIG: "{{.TALOS_CONFIG}}"

tasks:
  # =============================================================================
  # Default
  # =============================================================================
  default:
    desc: Show available tasks
    cmds:
      - task --list

  # =============================================================================
  # Talos - Configuration
  # =============================================================================
  talos:genconfig:
    desc: Generate Talos configs from talconfig.yaml
    dir: "{{.TALOS_DIR}}"
    cmds:
      - talhelper genconfig
    sources:
      - talconfig.yaml
      - talsecret.sops.yaml
    generates:
      - clusterconfig/*.yaml

  # =============================================================================
  # Talos - Fresh Install (insecure mode)
  # =============================================================================
  talos:apply-insecure:
    desc: Apply config to a node in insecure mode (fresh install)
    requires:
      vars: [NODE]
    vars:
      NODE_IP:
        sh: |
          case "{{.NODE}}" in
            luffy) echo "192.168.1.10" ;;
            zoro) echo "192.168.1.11" ;;
            nami) echo "192.168.1.12" ;;
            *) echo "{{.NODE}}" ;;
          esac
      NODE_NAME:
        sh: |
          case "{{.NODE}}" in
            192.168.1.10) echo "luffy" ;;
            192.168.1.11) echo "zoro" ;;
            192.168.1.12) echo "nami" ;;
            *) echo "{{.NODE}}" ;;
          esac
    cmds:
      - echo "Applying config to {{.NODE_NAME}} ({{.NODE_IP}}) in insecure mode..."
      - talosctl apply-config --nodes {{.NODE_IP}} --file {{.TALOS_DIR}}/clusterconfig/{{.CLUSTER_NAME}}-{{.NODE_NAME}}.yaml --insecure

  talos:apply-insecure-all:
    desc: Apply config to ALL nodes in insecure mode (fresh install)
    cmds:
      - task: talos:apply-insecure
        vars: { NODE: luffy }
      - task: talos:apply-insecure
        vars: { NODE: zoro }
      - task: talos:apply-insecure
        vars: { NODE: nami }
      - echo "Config applied to all nodes. Wait for them to reboot and install."

  # =============================================================================
  # Talos - Normal Operations
  # =============================================================================
  talos:apply:
    desc: Apply config to a node (e.g. task talos:apply NODE=luffy)
    requires:
      vars: [NODE]
    vars:
      NODE_IP:
        sh: |
          case "{{.NODE}}" in
            luffy) echo "192.168.1.10" ;;
            zoro) echo "192.168.1.11" ;;
            nami) echo "192.168.1.12" ;;
            *) echo "{{.NODE}}" ;;
          esac
      NODE_NAME:
        sh: |
          case "{{.NODE}}" in
            192.168.1.10) echo "luffy" ;;
            192.168.1.11) echo "zoro" ;;
            192.168.1.12) echo "nami" ;;
            *) echo "{{.NODE}}" ;;
          esac
    cmds:
      - talosctl apply-config --nodes {{.NODE_IP}} --file {{.TALOS_DIR}}/clusterconfig/{{.CLUSTER_NAME}}-{{.NODE_NAME}}.yaml

  talos:apply-all:
    desc: Apply config to all nodes
    cmds:
      - task: talos:apply
        vars: { NODE: luffy }
      - task: talos:apply
        vars: { NODE: zoro }
      - task: talos:apply
        vars: { NODE: nami }

  # =============================================================================
  # Talos - Bootstrap
  # =============================================================================
  talos:bootstrap:
    desc: Bootstrap the Kubernetes cluster (run once on first node)
    prompt: "Bootstrap Kubernetes cluster on luffy ({{.NODE_LUFFY}})? This should only be done ONCE."
    cmds:
      - echo "Bootstrapping Kubernetes cluster..."
      - talosctl bootstrap --nodes {{.NODE_LUFFY}}
      - echo "Kubernetes bootstrapped. Run 'task talos:kubeconfig' to get credentials."

  talos:kubeconfig:
    desc: Fetch kubeconfig from the cluster
    cmds:
      - talosctl kubeconfig --nodes {{.NODE_LUFFY}} --force
      - echo "Kubeconfig saved to ~/.kube/config"

  # =============================================================================
  # Talos - Reset
  # =============================================================================
  talos:reset-soft:
    desc: Reset cluster state but keep Talos installed (wipe Kubernetes only)
    prompt: "SOFT RESET - This will wipe Kubernetes state but keep Talos installed. Continue?"
    cmds:
      - echo "Resetting cluster state (keeping Talos)..."
      - talosctl reset --nodes {{.NODES}} --system-labels-to-wipe=STATE --system-labels-to-wipe=EPHEMERAL --graceful=false --reboot
      - echo ""
      - echo "Reset initiated. Nodes will reboot."
      - echo "After reboot run 'task talos:apply-insecure-all' then 'task talos:bootstrap' then 'task talos:kubeconfig' then 'task bootstrap:full'"

  talos:reset-hard:
    desc: Full reset - wipes everything including Talos (requires USB reinstall)
    prompt: "HARD RESET - This will COMPLETELY WIPE all disks including Talos. You will need USB reinstall. Continue?"
    cmds:
      - echo "Performing HARD reset (wiping everything)..."
      - talosctl reset --nodes {{.NODES}} --graceful=false --reboot
      - echo ""
      - echo "Nodes will boot to UEFI shell."
      - echo "Boot each node from Talos USB, then run the bootstrap tasks."

  # =============================================================================
  # Talos - Upgrades
  # =============================================================================
  talos:upgrade:
    desc: Upgrade Talos on a node (e.g. task talos:upgrade NODE=luffy)
    requires:
      vars: [NODE]
    vars:
      NODE_IP:
        sh: |
          case "{{.NODE}}" in
            luffy) echo "192.168.1.10" ;;
            zoro) echo "192.168.1.11" ;;
            nami) echo "192.168.1.12" ;;
            *) echo "{{.NODE}}" ;;
          esac
      TALOS_VERSION:
        sh: yq '.talosVersion' {{.TALOS_DIR}}/talconfig.yaml
      INSTALLER_IMAGE:
        sh: yq '.machine.install.image' {{.TALOS_DIR}}/clusterconfig/going-merry-{{.NODE}}.yaml | head -1
    preconditions:
      - sh: test -n "{{.INSTALLER_IMAGE}}"
        msg: "Cannot read installer image. Run 'task talos:genconfig' first."
    prompt: "Upgrade {{.NODE}} ({{.NODE_IP}}) to Talos {{.TALOS_VERSION}}?"
    cmds:
      - echo "Upgrading {{.NODE}} to Talos {{.TALOS_VERSION}}..."
      - talosctl upgrade --nodes {{.NODE_IP}} --image {{.INSTALLER_IMAGE}} --preserve --talosconfig {{.TALOS_DIR}}/clusterconfig/talosconfig
      - task: talos:wait-healthy
        vars: { NODE_IP: "{{.NODE_IP}}" }

  talos:upgrade-all:
    desc: Upgrade Talos on all nodes sequentially
    prompt: "Upgrade ALL nodes to the new Talos version?"
    cmds:
      - task: talos:upgrade
        vars: { NODE: luffy }
      - task: talos:upgrade
        vars: { NODE: zoro }
      - task: talos:upgrade
        vars: { NODE: nami }

  talos:upgrade-k8s:
    desc: Upgrade Kubernetes version
    vars:
      K8S_VERSION:
        sh: yq '.kubernetesVersion' {{.TALOS_DIR}}/talconfig.yaml
    prompt: "Upgrade Kubernetes to {{.K8S_VERSION}}?"
    cmds:
      - echo "Upgrading Kubernetes to {{.K8S_VERSION}}..."
      - talosctl upgrade-k8s --nodes {{.NODE_LUFFY}} --to {{.K8S_VERSION}}

  # =============================================================================
  # Talos - Status & Monitoring
  # =============================================================================
  talos:status:
    desc: Show Talos cluster status
    cmds:
      - echo "=== Talos Members ==="
      - talosctl get members --nodes {{.NODE_LUFFY}} 2>/dev/null || echo "Cannot reach cluster"
      - echo ""
      - echo "=== Kubernetes Nodes ==="
      - kubectl get nodes -o wide 2>/dev/null || echo "Cannot reach Kubernetes API"

  talos:health:
    desc: Check cluster health
    cmds:
      - talosctl health --nodes {{.NODES}}

  talos:dashboard:
    desc: Open interactive Talos dashboard
    cmds:
      - talosctl dashboard --nodes {{.NODES}}

  talos:wait-healthy:
    desc: Wait for a node to become healthy
    internal: true
    vars:
      NODE_IP: "{{.NODE_IP | default .NODE_LUFFY}}"
    cmds:
      - echo "Waiting for node {{.NODE_IP}} to be healthy..."
      - until talosctl health --nodes {{.NODE_IP}} 2>/dev/null; do sleep 5; done
      - echo "Node {{.NODE_IP}} is healthy"

  talos:wait-ready:
    desc: Wait for all nodes to be Ready in Kubernetes
    cmds:
      - echo "Waiting for all nodes to be Ready..."
      - kubectl wait --for=condition=ready node --all --timeout=300s
      - echo "All nodes are Ready"

  # =============================================================================
  # Bootstrap - Fresh Cluster Setup
  # =============================================================================
  bootstrap:helmfile:
    desc: Install Cilium CNI + Prometheus CRDs via helmfile (required before Flux)
    dir: bootstrap
    cmds:
      - echo "Updating Helm repositories..."
      - helm repo update
      - echo "Installing Cilium + Prometheus CRDs via helmfile..."
      - helmfile apply
      - echo "Waiting for Cilium pods to be created..."
      - sleep 30
      - echo "Waiting for Cilium to be ready..."
      - kubectl -n kube-system wait --for=condition=ready pod -l k8s-app=cilium --timeout=300s
      - echo "Waiting for all nodes to be Ready..."
      - kubectl wait --for=condition=ready node --all --timeout=300s
      - echo "Bootstrap complete - Cilium CNI + Prometheus CRDs installed"

  bootstrap:cilium:
    desc: "[DEPRECATED] Use bootstrap:helmfile instead"
    cmds:
      - echo "This task is deprecated. Use 'task bootstrap:helmfile' instead."
      - task: bootstrap:helmfile

  bootstrap:sops-secret:
    desc: Create SOPS Age secret for Flux
    cmds:
      - kubectl create namespace flux-system --dry-run=client -o yaml | kubectl apply -f -
      - |
        if kubectl -n flux-system get secret sops-age &>/dev/null; then
          echo "SOPS secret already exists"
          exit 0
        fi
        AGE_KEY=""
        if [ -f "$HOME/Library/Application Support/sops/age/keys.txt" ]; then
          AGE_KEY="$HOME/Library/Application Support/sops/age/keys.txt"
        elif [ -f "$HOME/.config/sops/age/keys.txt" ]; then
          AGE_KEY="$HOME/.config/sops/age/keys.txt"
        fi
        if [ -z "$AGE_KEY" ]; then
          echo "Age key not found. Create SOPS secret manually."
          exit 1
        fi
        echo "Creating SOPS Age secret..."
        cat "$AGE_KEY" | kubectl create secret generic sops-age --namespace=flux-system --from-file=age.agekey=/dev/stdin
        echo "SOPS secret created"

  bootstrap:flux:
    desc: Bootstrap Flux GitOps (after Cilium)
    preconditions:
      - sh: kubectl get nodes -o jsonpath='{.items[0].status.conditions[?(@.type=="Ready")].status}' 2>/dev/null | grep -q True
        msg: "Nodes are not Ready. Run 'task bootstrap:cilium' first."
    cmds:
      - task: bootstrap:sops-secret
      - |
        if [ -z "$GITHUB_TOKEN" ]; then
          echo "GITHUB_TOKEN not set. Run 'export GITHUB_TOKEN=your-token' first."
          exit 1
        fi
      - echo "Bootstrapping Flux..."
      - flux bootstrap github --owner=bruno00o --repository=homelab --branch=main --path=kubernetes/flux --personal
      - echo "Flux bootstrapped"

  bootstrap:full:
    desc: Full cluster bootstrap (Helmfile + Flux)
    cmds:
      - task: bootstrap:helmfile
      - task: bootstrap:flux
      - echo ""
      - echo "Cluster fully bootstrapped!"
      - echo "Watch progress with 'flux get kustomizations -w'"

  # =============================================================================
  # Flux
  # =============================================================================
  flux:reconcile:
    desc: Force Flux reconciliation
    cmds:
      - flux reconcile source git flux-system
      - flux reconcile kustomization flux-system --with-source
      - flux reconcile kustomization cluster-vars --with-source
      - flux reconcile kustomization infrastructure --with-source
      - flux reconcile kustomization apps --with-source

  flux:status:
    desc: Show Flux status
    cmds:
      - flux get all -A

  flux:logs:
    desc: Show Flux controller logs
    cmds:
      - flux logs --all-namespaces --since=10m

  flux:hr:
    desc: List all HelmReleases
    cmds:
      - kubectl get helmreleases -A

  flux:suspend:
    desc: Suspend all Flux reconciliation
    prompt: "Suspend all Flux kustomizations?"
    cmds:
      - flux suspend kustomization --all -n flux-system

  flux:resume:
    desc: Resume all Flux reconciliation
    cmds:
      - flux resume kustomization --all -n flux-system

  # =============================================================================
  # SOPS
  # =============================================================================
  sops:encrypt:
    desc: Encrypt a secret file (e.g. task sops:encrypt FILE=secret.sops.yaml)
    requires:
      vars: [FILE]
    dir: kubernetes
    cmds:
      - sops --encrypt --in-place {{.FILE}}

  sops:decrypt:
    desc: Decrypt a file for reading
    requires:
      vars: [FILE]
    dir: kubernetes
    cmds:
      - sops --decrypt {{.FILE}}

  sops:edit:
    desc: Edit an encrypted file
    requires:
      vars: [FILE]
    dir: kubernetes
    cmds:
      - sops {{.FILE}}

  # =============================================================================
  # Kubernetes - Quick Commands
  # =============================================================================
  k:pods:
    desc: List all pods
    cmds:
      - kubectl get pods -A

  k:events:
    desc: Show recent cluster events
    cmds:
      - kubectl get events -A --sort-by='.lastTimestamp' | tail -30

  k:top:
    desc: Show resource usage
    cmds:
      - echo "=== Nodes ==="
      - kubectl top nodes
      - echo ""
      - echo "=== Top Pods by Memory ==="
      - kubectl top pods -A --sort-by=memory | head -15

  k:errors:
    desc: Show pods with errors
    cmds:
      - kubectl get pods -A | grep -v Running | grep -v Completed

  # =============================================================================
  # Dependencies
  # =============================================================================
  deps:check:
    desc: Check if required tools are installed
    cmds:
      - |
        echo "Checking dependencies..."
        MISSING=""
        command -v talosctl >/dev/null || MISSING="$MISSING talosctl"
        command -v talhelper >/dev/null || MISSING="$MISSING talhelper"
        command -v kubectl >/dev/null || MISSING="$MISSING kubectl"
        command -v flux >/dev/null || MISSING="$MISSING flux"
        command -v helm >/dev/null || MISSING="$MISSING helm"
        command -v helmfile >/dev/null || MISSING="$MISSING helmfile"
        command -v sops >/dev/null || MISSING="$MISSING sops"
        command -v yq >/dev/null || MISSING="$MISSING yq"
        command -v jq >/dev/null || MISSING="$MISSING jq"
        if [ -n "$MISSING" ]; then
          echo "Missing:$MISSING"
          echo "Run 'task deps:install'"
          exit 1
        fi
        echo "All dependencies installed"

  deps:install:
    desc: Install all dependencies via Homebrew
    cmds:
      - brew install siderolabs/tap/talosctl
      - brew install talhelper
      - brew install kubectl
      - brew install fluxcd/tap/flux
      - brew install helm
      - brew install helmfile
      - brew install sops age
      - brew install yq jq
      - echo "All dependencies installed"

  # =============================================================================
  # CNPG - CloudNativePG
  # =============================================================================
  cnpg:status:
    desc: Show CNPG clusters status
    cmds:
      - echo "=== CNPG Clusters ==="
      - kubectl get clusters.postgresql.cnpg.io -A
      - echo "=== Recent CNPG Backups ==="
      - kubectl get backups.postgresql.cnpg.io -A --sort-by='.metadata.creationTimestamp' | tail -10

  # =============================================================================
  # Disaster Recovery - Longhorn Restore
  # =============================================================================
  dr:list-backups:
    desc: List available Longhorn backups from NAS
    cmds:
      - |
        echo "=== Longhorn Backups from NAS ==="
        kubectl get backups.longhorn.io -n longhorn-system -o json | \
          jq -r '.items[] | select(.status.state == "Completed") | (.status.labels.KubernetesStatus | fromjson) as $k | "\($k.namespace)/\($k.pvcName) | \(.status.volumeSize | tonumber / 1073741824 | floor)GB | \(.status.backupCreatedAt) | \(.metadata.name)"' | \
          sort | uniq | column -t -s'|'
        echo ""
        echo "To restore a specific PVC, use:"
        echo "  task dr:restore PVC=<namespace>/<pvc-name>"

  dr:restore:
    desc: Restore a PVC from Longhorn backup (e.g. task dr:restore PVC=authentik/data-authentik-postgres-1)
    requires:
      vars: [PVC]
    vars:
      NAMESPACE: '{{index (splitList "/" .PVC) 0}}'
      PVC_NAME: '{{index (splitList "/" .PVC) 1}}'
    prompt: "Restore {{.PVC}} from backup? This will create a new volume from the latest backup."
    cmds:
      - echo "=== Restoring {{.PVC}} ==="
      - echo "1. Finding latest backup for {{.PVC}}..."
      - |
        # Find the backup URL for this PVC
        BACKUP_URL=$(kubectl get backups.longhorn.io -n longhorn-system -o json | jq -r --arg ns "{{.NAMESPACE}}" --arg pvc "{{.PVC_NAME}}" '
          .items[] |
          select(.status.state == "Completed") |
          select((.status.labels.KubernetesStatus | fromjson | .namespace) == $ns) |
          select((.status.labels.KubernetesStatus | fromjson | .pvcName) == $pvc) |
          .status.url
        ' | tail -1)

        if [ -z "$BACKUP_URL" ]; then
          echo "❌ No backup found for {{.PVC}}"
          exit 1
        fi

        echo "   Found backup: $BACKUP_URL"

        # Extract volume size
        VOLUME_SIZE=$(kubectl get backups.longhorn.io -n longhorn-system -o json | jq -r --arg ns "{{.NAMESPACE}}" --arg pvc "{{.PVC_NAME}}" '
          .items[] |
          select(.status.state == "Completed") |
          select((.status.labels.KubernetesStatus | fromjson | .namespace) == $ns) |
          select((.status.labels.KubernetesStatus | fromjson | .pvcName) == $pvc) |
          .status.volumeSize
        ' | tail -1)

        SIZE_GI=$((VOLUME_SIZE / 1073741824))
        echo "   Volume size: ${SIZE_GI}Gi"
        echo ""

        # Generate volume name
        VOLUME_NAME="restored-{{.PVC_NAME}}-$(date +%s)"

        echo "2. Creating Longhorn volume from backup..."
        kubectl apply -f - <<EOF
        apiVersion: longhorn.io/v1beta2
        kind: Volume
        metadata:
          name: $VOLUME_NAME
          namespace: longhorn-system
        spec:
          numberOfReplicas: 3
          dataEngine: v1
          frontend: blockdev
          fromBackup: "$BACKUP_URL"
          size: "$VOLUME_SIZE"
        EOF

        echo "   ✅ Volume $VOLUME_NAME created"
        echo ""

        echo "3. Waiting for volume to be ready..."
        until kubectl get volumes.longhorn.io -n longhorn-system $VOLUME_NAME -o jsonpath='{.status.state}' 2>/dev/null | grep -q "detached"; do
          STATE=$(kubectl get volumes.longhorn.io -n longhorn-system $VOLUME_NAME -o jsonpath='{.status.state}' 2>/dev/null || echo "creating")
          echo "   State: $STATE"
          sleep 5
        done
        echo "   ✅ Volume ready"
        echo ""

        echo "4. Creating PV..."
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: $VOLUME_NAME
        spec:
          capacity:
            storage: ${SIZE_GI}Gi
          volumeMode: Filesystem
          accessModes:
            - ReadWriteOnce
          persistentVolumeReclaimPolicy: Retain
          storageClassName: longhorn
          csi:
            driver: driver.longhorn.io
            volumeHandle: $VOLUME_NAME
            fsType: ext4
        EOF
        echo "   ✅ PV created"
        echo ""

        echo "5. Creating PVC..."
        kubectl create namespace {{.NAMESPACE}} --dry-run=client -o yaml | kubectl apply -f -
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: {{.PVC_NAME}}
          namespace: {{.NAMESPACE}}
        spec:
          accessModes:
            - ReadWriteOnce
          storageClassName: longhorn
          volumeName: $VOLUME_NAME
          resources:
            requests:
              storage: ${SIZE_GI}Gi
        EOF
        echo "   ✅ PVC created"
        echo ""

        echo "=== Restore complete! ==="
        echo ""
        echo "Volume: $VOLUME_NAME"
        echo "PVC: {{.NAMESPACE}}/{{.PVC_NAME}}"
        echo ""
        echo "The workload using this PVC will automatically attach to the restored data."

  dr:restore-all:
    desc: List all PVCs that can be restored (run after hard reset)
    cmds:
      - |
        echo "=== Available PVC Restorations ==="
        echo "Run these commands to restore your data:"
        echo ""
        kubectl get backups.longhorn.io -n longhorn-system -o json 2>/dev/null | \
          jq -r '.items[] | select(.status.state == "Completed") | (.status.labels.KubernetesStatus | fromjson) as $k | "task dr:restore PVC=\($k.namespace)/\($k.pvcName)"' | \
          sort | uniq || echo "No backups found - is Longhorn running and connected to NAS?"
        echo ""
        echo "Note: Restore PVCs BEFORE Flux creates the workloads, or suspend Flux first."

  # =============================================================================
  # Cleanup
  # =============================================================================
  clean:
    desc: Clean generated Talos config files
    prompt: "Delete generated config files?"
    cmds:
      - rm -f {{.TALOS_DIR}}/clusterconfig/{{.CLUSTER_NAME}}-*.yaml
      - rm -f {{.TALOS_DIR}}/clusterconfig/talosconfig
      - echo "Generated files removed"
